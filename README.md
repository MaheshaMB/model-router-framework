üöÄ **Model Router Framework**


A smart decision-engine for automated LLM selection ‚Äî solving the challenge of manual model choice, cost inefficiency, throttling failures, and inconsistent behavior across multi-provider AI systems .


üß≠ **It dynamically selects the right LLM or embedding model based on:**

* Task Type ‚ûù Chat / Embedding
* Fallback Policy ‚ûù Automatic retry & switch when **throttled exception**
* Query Size ‚ûù Token length + context window
* Complexity ‚ûù Shallow answer vs deep reasoning
* Language ‚ûù English / Multilingual
* Cost Tier ‚ûù Low / Medium / High budget governance
* Tenant Tier ‚ûù Free / Standard / Premium multi-tenant control
* Fallback Policy ‚ûù Automatic retry & switch when throttled

‚öôÔ∏è **Router Behaviors**

‚úî Automatically estimates token size

‚úî Detects language (English vs multilingual)

‚úî Evaluates complexity heuristically

‚úî Validates model context/embedding limits

‚úî Applies cost/tenant policy rules

‚úî Retry w/ exponential backoff on throttling

‚úî Failover to backup model if primary is throttled

‚úî Transparent to calling app

üèó **Architecture**

        Application / Agent / RAG
                ‚îÇ
                ‚ñº
        ModelRouter.select_model()   ‚Üê  Tenant + metadata
                ‚îÇ
                ‚ñº
        Feature Extraction (task/size/lang/complexity/context)
                ‚îÇ
                ‚ñº
        Routing Rules (fetch rule.json from S3 / local)
                ‚îÇ
                ‚ñº
        Model Config Selection (fetch modle.json from S3 / local)
                ‚îÇ
                ‚ñº
        Provider Client (Bedrock / Claude / Gemini) + Retries + Backup Failover
                ‚îÇ
                ‚ñº
        ModelHandle.chat() / ModelHandle.embed()


üß© **Installation**

1. Clone the repository
   
        git clone https://github.com/MaheshaMB/model-router-framework.git
        cd model-router-framework

3. Install dependencies
  
        pip install -r requirements.txt

‚öôÔ∏è **Runtime Configuration**

For local JSON mode: 

        export FETCH_DRIVE=false

Or enable S3 configuration in production:

        export FETCH_DRIVE=true
        export MODEL_ROUTER_CONFIG_BUCKET=my-router-config
        export MODEL_ROUTER_MODELS_KEY=models.json
        export MODEL_ROUTER_RULES_KEY=routing_rules.json

Also set required LLM SDK credentials:

        export AWS_REGION=us-east-1
        export AWS_ACCESS_KEY_ID=your_key
        export AWS_SECRET_ACCESS_KEY=your_key
        export ANTHROPIC_API_KEY=your_key
        export GOOGLE_API_KEY=your_key


üöÄ **Usage Example**

Chat

        from model_router_framework import ModelRouter
        
        router = ModelRouter()

        # Simple user query
        query = "What is the llm model router ?"
        handle = router.select_model(text=query)
        response = handle.chat([{"role": "user", "content": query}])
        print(response)

        # Complex user query
        query = "Explain in detail of software architecture design in 20 pages"
        handle = router.select_model(text=query, task_type="chat")
        response = handle.chat([{"role": "user", "content": query}])
        print(response)


Embedding

        from model_router_framework import ModelRouter
        
        router = ModelRouter()
        chunk = "Explain in detail of software architecture design in 20 pages"
        handle = router.select_model(text=chunk, task_type="embedding")
        response = handle.embed(chunk)
        print(response)


ü•á **Status**

* This is currently a POC implementation, and it can be evolved into an MVP and further to a production-ready solution by applying the necessary configuration enhancements and environment-specific adjustments.

* This solution is designed specifically for your application‚Äôs LLM call workflow, including RAG and Agentic AI flows. 

* It can be packaged as a reusable library and integrated directly into your application wherever LLM initialization and call execution are performed.
  
 
üôå **Contributions Welcome**
Issues, improvements, additional provider support, and real-world routing rules are encouraged!
 
 
üí° **Author Note**
This project aims to simplify llm-call, Agentic-AI, Enterprise RAG adoption by removing the complexity of selecting and managing multiple LLMs behind the scenes.
If you are working in AI Platform Engineering, Multi-LLM Systems, or Serverless AI, let‚Äôs collaborate! 
